https://www.tutorialspoint.com/data_structures_algorithms/asymptotic_analysis.htm

Asymptotic Notations

    Asymptotic Notations give us an idea about how good a given algorithm is compared to some other algorithm.
    
    Using asymptotic analysis, we can very well conclude the best case, average case, and worst case scenario of an algorithm.

Basically there are three types of widely used asymptotic notations.

    1. Big Oh notation (O)
    2. Big Omega notation(Ω)
    3. Big theta notation(θ) ---> widely used one!

Big Oh(O): Asymptotic Upper Bound

    It measures the worst case time complexity

    A function f(n) can be represented is the order of g(n) that is O(g(n)), if there exists a value of positive integer n as n0 and a positive constant c such that −

    f(n)⩽c.g(n)for n>n0 in all case

    or, If f(n) describes running time of an algorithm; f(n) is O(g(n)) if there exists positive constants C and n0(n not) such that 
    0<=f(n) <=Cg(n) for all n>=n0 ; here n used to give upper bound on a function
    If a function is O(n), it is automatically O(n^2) as well

Hence, function g(n) is an upper bound for function f(n), as g(n) grows faster than f(n).

Example

    Let us consider a given function, f(n)=4.n^3+10.n^2+5.n+1
    Considering g(n)=n3
    ,

    f(n)⩽5.g(n)
    for all the values of n>2
    Hence, the complexity of f(n) can be represented as O(g(n))
    , i.e. O(n3)

Big Omega, Ω: Asymptotic Lower Bound

    It measures the best case time complexity.Just like O notation provide asymptotic upper bound, Ω notation provide asymptotic lower bound. Let f(n) define running time of an algorithm;

    f(n) is said to be Ω(gn) if there exists positive constant C and n0(n not) such that O⩽Cg(n)⩽f(n) for all n⩾n0

    If a function is O(n^2) it is automatically O(n) as well

Example
    Let us consider a given function, f(n)=4.n^3+10.n^2+5.n+1.

    Considering g(n)=n3, f(n)⩾4.g(n)for all the values of n>0.

    Hence, the complexity of f(n) can be represented as Ω(g(n))
    , i.e. Ω(n3)

Theta, θ: Asymptotic Tight Bound
    The notation θ(n) is the formal way to express both the lower bound and the upper bound of an algorithm's running time.

    We say that f(n)=θ(g(n))
    when there exist constants c1 and c2 that c1.g(n)⩽f(n)⩽c2.g(n)
    for all sufficiently large value of n. Here n is a positive integer.

    This means function g is a tight bound for function f.

    The equation simply means there exist positive constant C1 and C2 such that f(x) is sandwiched between C2g(n) and C1 g(n)

Which one of these to use ?
    Since Big theta gives a better picture of runtime for a given algorithm, most of the interviewers expect you to provide an answer in terms of Big theta they say "Order of".

Increasing order of Common runtimes 

    1 < log n < n < nlog n < n^2 < n^3 < 2^n < n^n
  Better         common runtime              worse
                to beter to worse

